{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datajoedata/transcricao-pipeline-monitor-rosa/blob/main/notebook_cancer_mama_kpis_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNviCQI6IDqh"
      },
      "source": [
        "# I - Configuração do ambiente\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqola1AcrdGm"
      },
      "source": [
        "## 1.1 - Montagem do drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enmrocYf_raz",
        "outputId": "50bf7b62-e6bf-43a2-d691-ff19535f8c1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnBLAcn1ZEik"
      },
      "source": [
        "## 1.2 - Escolha do ambiente e carregamento\n",
        "São disponibilizadas duas opções de execução (dependendo da credencial utilizada)\n",
        " - Leitura: disponível para analises e usuários que não possuem permissão de escrita;\n",
        " - Escrita: disponível para atualização de bases (testes de desenvolvimento ou ambiente de produção)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "Fj3xNzUzZD8W",
        "outputId": "70db9df5-0642-44ca-ed51-b2dba00b0ace"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: SERVICE_ACCOUNT_USER=''\n",
            "env: SERVICE_ACCOUNT_JSON=''\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'shared_drive'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import os\n",
        "if os.path.isfile('/content/monitor-rosa-leitura.json'):\n",
        "    datalake_mode = 'leitura'\n",
        "    %env SERVICE_ACCOUNT_USER=acesso-leitura@monitor-rosa.iam.gserviceaccount.com\n",
        "    %env SERVICE_ACCOUNT_JSON=/content/monitor-rosa-leitura.json\n",
        "elif os.path.isfile('/content/monitor-rosa-escrita.json'):\n",
        "    datalake_mode = 'escrita'\n",
        "    %env SERVICE_ACCOUNT_USER=acesso-escrita@monitor-rosa.iam.gserviceaccount.com\n",
        "    %env SERVICE_ACCOUNT_JSON=/content/monitor-rosa-escrita.json\n",
        "else:\n",
        "    assert(os.path.isdir('/content/drive/Shareddrives/monitor-rosa-gold') == True)\n",
        "    datalake_mode = 'shared_drive'\n",
        "    %env SERVICE_ACCOUNT_USER=''\n",
        "    %env SERVICE_ACCOUNT_JSON=''\n",
        "datalake_mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxl5dgNHuCsK",
        "outputId": "c1418d0b-c2c5-462d-f8e0-074820e7bc21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'sus-kpis-analysis': No such file or directory\n",
            "Cloning into 'sus-kpis-analysis'...\n",
            "remote: Enumerating objects: 1735, done.\u001b[K\n",
            "remote: Counting objects: 100% (271/271), done.\u001b[K\n",
            "remote: Compressing objects: 100% (171/171), done.\u001b[K\n",
            "remote: Total 1735 (delta 106), reused 204 (delta 83), pack-reused 1464 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1735/1735), 4.72 MiB | 12.78 MiB/s, done.\n",
            "Resolving deltas: 100% (807/807), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -r sus-kpis-analysis\n",
        "!git clone https://github.com/heber-augusto/sus-kpis-analysis.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NYsSZynDK9K"
      },
      "source": [
        "## 1.3 - Instalação de libs Python, inicialização de variáveis de ambiente e configuração/instalação do Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v3Wqjxhu4Gq",
        "outputId": "8706731c-3b60-471c-a395-4cce46161078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark (from -r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 1))\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Collecting pyspark (from -r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 2))\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting imagehash (from -r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 3))\n",
            "  Downloading ImageHash-4.3.1-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting delta-spark==2.4.0 (from -r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 4))\n",
            "  Downloading delta_spark-2.4.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (from -r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (2.8.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from -r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 6)) (2024.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 7)) (5.9.5)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from -r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 8)) (2.137.0)\n",
            "Collecting pyspark (from -r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 2))\n",
            "  Downloading pyspark-3.4.3.tar.gz (311.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from delta-spark==2.4.0->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 4)) (8.4.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 2)) (0.10.9.7)\n",
            "Collecting PyWavelets (from imagehash->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 3))\n",
            "  Downloading pywavelets-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imagehash->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 3)) (1.26.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from imagehash->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 3)) (10.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from imagehash->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (2.27.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (2.19.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (2.7.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (2.32.3)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 8)) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 8)) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 8)) (4.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (1.65.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (3.20.3)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (1.24.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (1.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 8)) (3.1.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.0.0->delta-spark==2.4.0->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 4)) (3.20.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (2024.8.30)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (0.6.1)\n",
            "Downloading delta_spark-2.4.0-py3-none-any.whl (20 kB)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Downloading ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.5/296.5 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pywavelets-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.3-py2.py3-none-any.whl size=311885495 sha256=d1de8baab2a979f74e599c85e1d37bc68fb99f2e7f389def7605247eb69f2c26\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/a9/64/3713eb2c5048c18bae2778b013e5fc74203f5c22d4640fb776\n",
            "Successfully built pyspark\n",
            "Installing collected packages: findspark, PyWavelets, pyspark, imagehash, delta-spark\n",
            "Successfully installed PyWavelets-1.7.0 delta-spark-2.4.0 findspark-2.0.1 imagehash-4.3.1 pyspark-3.4.3\n",
            "env: PYTHONHASHSEED=1234\n",
            "env: JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
            "env: SPARK_HOME=/content/spark-3.4.3-bin-hadoop3\n",
            "env: SPARK_VERSION=3.4.3\n",
            "starting spark env setup \n",
            "installing and downloading packages\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "setting enviroment variables\n",
            "spark env setup completed with success\n"
          ]
        }
      ],
      "source": [
        "!pip install -r /content/sus-kpis-analysis/sia/etls/requirements.txt\n",
        "\n",
        "%env PYTHONHASHSEED=1234\n",
        "%env JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
        "%env SPARK_HOME=/content/spark-3.4.3-bin-hadoop3\n",
        "%env SPARK_VERSION=3.4.3\n",
        "\n",
        "!source /content/sus-kpis-analysis/sia/etls/bin/setup_spark_env.sh '/content/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_O3K75fMh7K"
      },
      "outputs": [],
      "source": [
        "if datalake_mode != 'shared_drive':\n",
        "    %env XDG_CONFIG_HOME=/content/datalake\n",
        "    !source /content/sus-kpis-analysis/sia/etls/bin/install-google-drive-ocamlfuse.sh\n",
        "    !source /content/sus-kpis-analysis/sia/etls/bin/mount_google_drive_v2.sh '/content/datalake' $SERVICE_ACCOUNT_USER '0ABIY-a4qrdY9Uk9PVA' 'monitor-rosa-bronze' $SERVICE_ACCOUNT_JSON '/content'\n",
        "    !source /content/sus-kpis-analysis/sia/etls/bin/mount_google_drive_v2.sh '/content/datalake' $SERVICE_ACCOUNT_USER '0ALl0owLNr53oUk9PVA' 'monitor-rosa-silver' $SERVICE_ACCOUNT_JSON '/content'\n",
        "    !source /content/sus-kpis-analysis/sia/etls/bin/mount_google_drive_v2.sh '/content/datalake' $SERVICE_ACCOUNT_USER '0AMHp9pBeLvZiUk9PVA' 'monitor-rosa-gold' $SERVICE_ACCOUNT_JSON '/content'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT00Lo1mTJwu"
      },
      "source": [
        "## 1.4 - Inicializa variáveis de acesso ao delta lake criado no google storage\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> O caminho do warehouse pode ser alterado em caso de testes de escritas locais.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LLT8gGOcekpW",
        "outputId": "d9b06258-035d-4e26-965b-ab43e18771fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/spark-3.4.3-bin-hadoop3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "lake_prefix = \"temp-output\"\n",
        "\n",
        "if datalake_mode in ('leitura','shared_drive',):\n",
        "    warehouse_dir = f\"/content/datalake/{lake_prefix}/\"\n",
        "\n",
        "if datalake_mode == 'escrita':\n",
        "    warehouse_dir = f\"/content/datalake/\"\n",
        "\n",
        "spark_path = os.getenv('SPARK_HOME')\n",
        "spark_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpzMUcNg3k8d"
      },
      "source": [
        "## 1.5 - Inclusão da pasta do repositório no python path\n",
        "\n",
        "Procedimento permite que funções e classes presentes no repositório sejam utilizadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-dOrv8t0IMZ",
        "outputId": "7dbe3773-bb3b-458c-aa6d-b04d540f6635"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content',\n",
              " '/env/python',\n",
              " '/usr/lib/python310.zip',\n",
              " '/usr/lib/python3.10',\n",
              " '/usr/lib/python3.10/lib-dynload',\n",
              " '',\n",
              " '/usr/local/lib/python3.10/dist-packages',\n",
              " '/usr/lib/python3/dist-packages',\n",
              " '/usr/local/lib/python3.10/dist-packages/IPython/extensions',\n",
              " '/usr/local/lib/python3.10/dist-packages/setuptools/_vendor',\n",
              " '/root/.ipython',\n",
              " '/content/sus-kpis-analysis']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('/content/sus-kpis-analysis')\n",
        "sys.path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptGWtMYSIXtw"
      },
      "source": [
        "## 1.6 - Importação de funções utilizadas pelo código"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYbxgrEQz3Sn"
      },
      "outputs": [],
      "source": [
        "from sia.etls.lib.catalog_loader import DeltaLakeDatabaseFsCreator, load_entire_catalog_fs, load_entire_catalog_fs_v2\n",
        "from sia.etls.lib.table_utilities import vacuum_tables_from_database, table_exists\n",
        "from sia.etls.lib.fs_spark_session import create_fs_spark_session\n",
        "from sia.etls.lib.bronze_files_utilities import get_pending_files_from_bronze\n",
        "from sia.etls.lib.delta_table_creators import ParquetToDelta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LWOtjtk33d6"
      },
      "source": [
        "## 1.7 - Cria Sessão Spark conectada ao Delta Lake presente no Google Storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETM5T_gkDvAP"
      },
      "outputs": [],
      "source": [
        "spark = create_fs_spark_session(\n",
        "    warehouse_dir=warehouse_dir,\n",
        "    spark_path=spark_path\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8lBJqvC4Cak"
      },
      "source": [
        "## 1.8 - Refresh do catálogo para utilizar consultas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvYjAMOAyNmj",
        "outputId": "76f72e19-203c-435c-b028-97ea6b8bb170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sia_bronze.db', 'cnes_bronze.db', 'sih_bronze.db', 'sim_bronze.db']\n",
            "Banco de dados sia_bronze criado.\n",
            "listando conteúdos do caminho /content/drive/Shareddrives/monitor-rosa-bronze/databases e database sia_bronze\n",
            "prefix: /content/drive/Shareddrives/monitor-rosa-bronze/databases/sia_bronze.db/\n",
            "table_list: ['ar', 'aq', 'pa', 'bi', 'am']\n",
            "Tabela ar criada\n",
            "Tabela ar criada com comando CREATE TABLE IF NOT EXISTS sia_bronze.ar USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-bronze/databases/sia_bronze.db/ar'\n",
            "Tabela aq criada\n",
            "Tabela aq criada com comando CREATE TABLE IF NOT EXISTS sia_bronze.aq USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-bronze/databases/sia_bronze.db/aq'\n",
            "Recriação das tabelas concluída.\n",
            "Banco de dados cnes_bronze criado.\n",
            "listando conteúdos do caminho /content/drive/Shareddrives/monitor-rosa-bronze/databases e database cnes_bronze\n",
            "prefix: /content/drive/Shareddrives/monitor-rosa-bronze/databases/cnes_bronze.db/\n",
            "table_list: ['dc', 'lt', 'ep', 'pf', 'eq', 'hb', 'in', 'rc', 'sr']\n",
            "Recriação das tabelas concluída.\n",
            "Banco de dados sih_bronze criado.\n",
            "listando conteúdos do caminho /content/drive/Shareddrives/monitor-rosa-bronze/databases e database sih_bronze\n",
            "prefix: /content/drive/Shareddrives/monitor-rosa-bronze/databases/sih_bronze.db/\n",
            "table_list: ['rd']\n",
            "Recriação das tabelas concluída.\n",
            "Banco de dados sim_bronze criado.\n",
            "listando conteúdos do caminho /content/drive/Shareddrives/monitor-rosa-bronze/databases e database sim_bronze\n",
            "prefix: /content/drive/Shareddrives/monitor-rosa-bronze/databases/sim_bronze.db/\n",
            "table_list: ['dores']\n",
            "Recriação das tabelas concluída.\n",
            "['cancer_data.db']\n",
            "Banco de dados cancer_data criado.\n",
            "listando conteúdos do caminho /content/drive/Shareddrives/monitor-rosa-silver/databases e database cancer_data\n",
            "prefix: /content/drive/Shareddrives/monitor-rosa-silver/databases/cancer_data.db/\n",
            "table_list: ['aq_filtered', 'ar_filtered', 'dados_estados_mensal', 'dados_municipios_mensal', 'pacientes', 'procedimentos', 'procedimentos_e_pacientes', 'demografia_municipios', 'cadastro_municipios']\n",
            "Tabela demografia_municipios criada\n",
            "Tabela demografia_municipios criada com comando CREATE TABLE IF NOT EXISTS cancer_data.demografia_municipios USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-silver/databases/cancer_data.db/demografia_municipios'\n",
            "Tabela cadastro_municipios criada\n",
            "Tabela cadastro_municipios criada com comando CREATE TABLE IF NOT EXISTS cancer_data.cadastro_municipios USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-silver/databases/cancer_data.db/cadastro_municipios'\n",
            "Recriação das tabelas concluída.\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "zone_names = ['monitor-rosa-bronze', 'monitor-rosa-silver', 'monitor-rosa-gold']\n",
        "if datalake_mode in ('leitura', 'escrita'):\n",
        "    zone_paths = [f'/content/datalake/{zone_name}/databases' for zone_name in zone_names]\n",
        "else:\n",
        "    zone_paths = [f'/content/drive/Shareddrives/{zone_name}/databases' for zone_name in zone_names]\n",
        "\n",
        "\n",
        "# carrega catalogo de banco de dados, na zona bronze\n",
        "database_filter = None #['cnes_bronze.db',]\n",
        "table_filter = ['sia_bronze.ar','sia_bronze.aq','cancer_data.cadastro_municipios', 'cancer_data.demografia_municipios']\n",
        "\n",
        "for databases_path in zone_paths:\n",
        "    load_entire_catalog_fs_v2(\n",
        "        spark_session = spark,\n",
        "        databases_path = databases_path,\n",
        "        use_db_folder_path=(datalake_mode == 'escrita'),\n",
        "        database_filter=database_filter,\n",
        "        table_filter=table_filter\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeQru0rTIfct"
      },
      "source": [
        "# II - Exemplo de como listar bancos e tabelas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJybgzFHU5as",
        "outputId": "cd4b5be0-350a-45d5-c767-19d62aa9c085"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|namespace|\n",
            "+---------+\n",
            "|  default|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "databases = spark.sql(f\"SHOW DATABASES;\")\n",
        "databases.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYbmd2NNH3iZ",
        "outputId": "85eb3c25-760a-4cf9-91a8-215cb4a6c070"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+---------+-----------+\n",
            "|namespace|tableName|isTemporary|\n",
            "+---------+---------+-----------+\n",
            "+---------+---------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for row in databases.collect():\n",
        "    spark.sql(f\"SHOW TABLES FROM {row['namespace']};\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy88IuwDyjFQ"
      },
      "source": [
        "# III - Extração e Filtragem:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa-XADxhEWvE"
      },
      "source": [
        "## 3.1 - Definindo utils e filtros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_joP1K-LEbIQ"
      },
      "outputs": [],
      "source": [
        "# Função para gerar consultas SQL\n",
        "def get_select_all_query(table_name, where_clause=''):\n",
        "    return f\"\"\"\n",
        "    SELECT\n",
        "        *\n",
        "    FROM {table_name}\n",
        "    {where_clause}\n",
        "    \"\"\"\n",
        "\n",
        "# Função para executar consultas SQL no Spark\n",
        "def run_sql_query(sql_query):\n",
        "    return spark.sql(sql_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rfb1iT7fEnCh",
        "outputId": "754113dc-dcb9-4db2-ec3e-b2b09a49e7f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('0201010569','0201010585','0201010607','0203010035','0203010043','0203020065','0203020073','0205020097','0208090037','0204030030','0204030188')\n",
            "('C500','C501','C502','C503','C504','C505','C506','C508','C509')\n"
          ]
        }
      ],
      "source": [
        "# Filtro pelo CID\n",
        "cid_filter = ['C500', 'C501', 'C502', 'C503', 'C504', 'C505', 'C506', 'C508', 'C509']\n",
        "\n",
        "cid_filter = f\"\"\"({','.join([f\"'{cid_id}'\" for cid_id in cid_filter])})\"\"\"\n",
        "\n",
        "# Dicionário e filtro de procedimentos\n",
        "proc_id_dict = {\n",
        "    '0201010569': 'BIOPSIA/EXERESE DE NÓDULO DE MAMA',\n",
        "    '0201010585': 'PUNÇÃO ASPIRATIVA DE MAMA POR AGULHA FINA',\n",
        "    '0201010607': 'PUNÇÃO DE MAMA POR AGULHA GROSSA',\n",
        "    '0203010035': 'EXAME DE CITOLOGIA (EXCETO CERVICO-VAGINAL E DE MAMA)',\n",
        "    '0203010043': 'EXAME CITOPATOLOGICO DE MAMA',\n",
        "    '0203020065': 'EXAME ANATOMOPATOLOGICO DE MAMA - BIOPSIA',\n",
        "    '0203020073': 'EXAME ANATOMOPATOLOGICO DE MAMA - PECA CIRURGICA',\n",
        "    '0205020097': 'ULTRASSONOGRAFIA MAMARIA BILATERAL',\n",
        "    '0208090037': 'CINTILOGRAFIA DE MAMA (BILATERAL)',\n",
        "    '0204030030': 'MAMOGRAFIA',\n",
        "    '0204030188': 'MAMOGRAFIA BILATERAL PARA RASTREAMENTO'\n",
        "}\n",
        "proc_id_filter = f\"\"\"({','.join([f\"'{proc_id}'\" for proc_id in proc_id_dict.keys()])})\"\"\"\n",
        "\n",
        "# Mostrar os filtros\n",
        "print(proc_id_filter)\n",
        "print(cid_filter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeIwwmsSHTzv"
      },
      "source": [
        "## 3.2 - Carrega tabela SIA.AR filtrando dados de câncer de mama e procedimentos de interesse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t36KwFBRtY1k"
      },
      "outputs": [],
      "source": [
        "sql_query_aq = get_select_all_query(\n",
        "    table_name= 'sia_bronze.ar',\n",
        "    where_clause=f\"WHERE AP_CIDPRI IN {cid_filter}\" #AND AP_MVM = '202405'\n",
        ")\n",
        "\n",
        "cancer_ar_filtered = run_sql_query(sql_query_aq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx_GDZq62XTU"
      },
      "source": [
        "Escreve a tabela em .cancer_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLCBiUZQ2R2t"
      },
      "outputs": [],
      "source": [
        "cancer_ar_filtered\\\n",
        "      .repartition(1)\\\n",
        "      .write\\\n",
        "      .format(\"delta\")\\\n",
        "      .mode(\"overwrite\")\\\n",
        "      .saveAsTable(\"cancer_data.ar_filtered\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjhvBzKogaT9"
      },
      "source": [
        "## 3.3 - Carrega tabela SIA.AQ filtrando dados de câncer de mama e procedimentos de interesse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bIQolpgc_a9"
      },
      "outputs": [],
      "source": [
        "sql_query_aq = get_select_all_query(\n",
        "    table_name= 'sia_bronze.aq',\n",
        "    where_clause=f\"WHERE AP_CIDPRI IN {cid_filter}\" #AND AP_MVM = '202405'\n",
        ")\n",
        "\n",
        "cancer_aq_filtered = run_sql_query(sql_query_aq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShU0d5Fv2agE"
      },
      "source": [
        "Escreve a tabela em .cancer_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-Jga0Lpeq32"
      },
      "outputs": [],
      "source": [
        "cancer_aq_filtered\\\n",
        "      .repartition(1)\\\n",
        "      .write\\\n",
        "      .format(\"delta\")\\\n",
        "      .mode(\"overwrite\")\\\n",
        "      .saveAsTable(\"cancer_data.aq_filtered\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ncymjIK6NlR"
      },
      "source": [
        "# IV - Processamento dos Dados dos Pacientes e Procedimentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn0xcaY-j3E2"
      },
      "source": [
        "## 4.1 - Cria dados consolidados de pacientes e procedimentos (quimio e radioterapia)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mx39mQ5w4S6"
      },
      "source": [
        "Radioterapia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozvlA8On_dHJ",
        "outputId": "ea28d3fd-7a9f-4957-9c7f-934b6264429a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------------+------------+------+-----+---------+\n",
            "|  data|       paciente|estadiamento| custo|obito|municipio|\n",
            "+------+---------------+------------+------+-----+---------+\n",
            "|201304|{{|{}|{||           0| 232.0|    0|   355030|\n",
            "|201304|{|~||{{|           0| 990.0|    0|   352850|\n",
            "|201304|{|{~}{~|           0|2422.0|    0|   355030|\n",
            "+------+---------------+------------+------+-----+---------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cancer_ar_res = spark.sql(f\"\"\"\n",
        "SELECT\n",
        "    AP_CMP as data,\n",
        "    AP_CNSPCN as paciente,\n",
        "    AR_ESTADI as estadiamento,\n",
        "    DOUBLE(AP_VL_AP)  as custo,\n",
        "    INT(AP_OBITO) as obito,\n",
        "    AP_MUNPCN as municipio\n",
        "FROM cancer_data.ar_filtered\n",
        "\"\"\")\n",
        "cancer_ar_res.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlF0pvjRw74j"
      },
      "source": [
        "Quimio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Dyzqz4rGhmG",
        "outputId": "2d6ef07c-e8b5-4da4-c0ed-e6cd7a0f993b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------------+------------+------+-----+---------+\n",
            "|  data|       paciente|estadiamento| custo|obito|municipio|\n",
            "+------+---------------+------------+------+-----+---------+\n",
            "|202405|{|{{{|}|           3|1400.0|    0|   355030|\n",
            "|202405|{{{|}{~~}|           3|1400.0|    0|   354850|\n",
            "|202405|{{}{|{~|           3|1400.0|    0|   352050|\n",
            "+------+---------------+------------+------+-----+---------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cancer_aq_res = spark.sql(f\"\"\"\n",
        "SELECT\n",
        "    AP_CMP as data,\n",
        "    AP_CNSPCN as paciente,\n",
        "    AQ_ESTADI as estadiamento,\n",
        "    DOUBLE(AP_VL_AP)  as custo,\n",
        "    INT(AP_OBITO) as obito,\n",
        "    AP_MUNPCN as municipio\n",
        "FROM cancer_data.aq_filtered\n",
        "\"\"\")\n",
        "cancer_aq_res.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOgAevcCNa07"
      },
      "source": [
        "## 4.2 - Unifica os dados de radio e quimio consolidados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94M3aPpRNZeT"
      },
      "outputs": [],
      "source": [
        "df_union = cancer_aq_res.union(cancer_ar_res)\n",
        "\n",
        "df_union.createOrReplaceTempView(\"cancer_ordered\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYZcjpFJVtbT"
      },
      "outputs": [],
      "source": [
        "df_union\\\n",
        "  .repartition(1)\\\n",
        "  .write\\\n",
        "  .format(\"delta\")\\\n",
        "  .mode(\"overwrite\")\\\n",
        "  .saveAsTable(f\"cancer_data.procedimentos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s588c0EgN5pU"
      },
      "source": [
        "## 4.3 - Consolidando os dados por paciente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kymNuaRsg87T"
      },
      "outputs": [],
      "source": [
        "res_consolidado = spark.sql(\"\"\"\n",
        "SELECT\n",
        "    paciente,\n",
        "    FIRST(data) as data_primeiro_estadiamento,\n",
        "    LAST(data) as data_ultimo_estadiamento,\n",
        "    COUNT(1) as numero_procedimentos,\n",
        "    FIRST(estadiamento) as primeiro_estadiamento,\n",
        "    LAST(estadiamento) as ultimo_estadiamento,\n",
        "    MAX (estadiamento) as maior_estadiamento,\n",
        "    MIN (estadiamento) as menor_estadiamento,\n",
        "    SUM(custo) as custo_total,\n",
        "    MAX(obito) as indicacao_obito,\n",
        "    FIRST(municipio) as primeiro_municipio,\n",
        "    LAST(municipio) as ultimo_municipio\n",
        "FROM (SELECT * FROM cancer_ordered ORDER BY paciente, data)\n",
        "GROUP BY paciente\n",
        "\"\"\")\n",
        "\n",
        "res_consolidado\\\n",
        "  .repartition(1)\\\n",
        "  .write\\\n",
        "  .format(\"delta\")\\\n",
        "  .mode(\"overwrite\")\\\n",
        "  .saveAsTable(f\"cancer_data.pacientes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31EFS5RgWrPr"
      },
      "source": [
        "## 4.4 - Procedimentos e Pacientes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2319Trei2iY"
      },
      "outputs": [],
      "source": [
        "procedimentos_e_pacientes = spark.sql(f\"\"\"\n",
        "  SELECT\n",
        "      c.*,\n",
        "      p.data_primeiro_estadiamento,\n",
        "      p.data_ultimo_estadiamento,\n",
        "      p.primeiro_estadiamento,\n",
        "      p.maior_estadiamento,\n",
        "      p.ultimo_estadiamento,\n",
        "      p.custo_total,\n",
        "      p.primeiro_municipio,\n",
        "      p.ultimo_municipio,\n",
        "      p.indicacao_obito\n",
        "  FROM cancer_data.procedimentos AS c\n",
        "  FULL OUTER JOIN cancer_data.pacientes AS p\n",
        "  ON c.paciente = p.paciente\n",
        "\"\"\")\n",
        "\n",
        "procedimentos_e_pacientes\\\n",
        "  .repartition(1)\\\n",
        "  .write\\\n",
        "  .format(\"delta\")\\\n",
        "  .mode(\"overwrite\")\\\n",
        "  .saveAsTable(\"cancer_data.procedimentos_e_pacientes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZyJAon8CaMw"
      },
      "source": [
        "# V - Agregação por Município e Estado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRM6BlAT9xPY"
      },
      "source": [
        "## 5.1 - Consolida dados por municipio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60A9MQRpaCZk"
      },
      "outputs": [],
      "source": [
        "database_name = \"cancer_data\"\n",
        "\n",
        "diagnosticos_por_estadiamento_municipio = spark.sql(f\"\"\"\n",
        "    SELECT\n",
        "        primeiro_estadiamento,\n",
        "        data_primeiro_estadiamento AS data,\n",
        "        primeiro_municipio AS municipio,\n",
        "        COUNT(DISTINCT(paciente)) AS numero_diagnosticos\n",
        "    FROM {database_name}.pacientes\n",
        "    WHERE primeiro_estadiamento != ''\n",
        "    GROUP BY primeiro_estadiamento, data_primeiro_estadiamento, primeiro_municipio\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "diagnosticos_por_estadiamento_municipio\\\n",
        "    .repartition(1)\\\n",
        "    .write\\\n",
        "    .format(\"delta\")\\\n",
        "    .mode(\"overwrite\")\\\n",
        "    .saveAsTable(f\"{database_name}.diagnosticos_por_estadiamento_municipio\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbZFX_cNdwPt"
      },
      "source": [
        "## 5.2 - Consolida dados mensais por municipio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLwtdxLCdrTN"
      },
      "outputs": [],
      "source": [
        "dados_estad_municipio_mensal = spark.sql(f\"\"\"\n",
        "    SELECT\n",
        "        data,\n",
        "        municipio,\n",
        "        primeiro_estadiamento,\n",
        "        SUM(custo) AS custo_estadiamento,\n",
        "        COUNT(DISTINCT(paciente)) AS numero_pacientes,\n",
        "        SUM(DISTINCT(obito)) AS obitos,\n",
        "        SUM(DISTINCT(indicacao_obito)) AS obito_futuro,\n",
        "        COUNT(1) AS numero_procedimentos\n",
        "    FROM\n",
        "        (SELECT * FROM {database_name}.procedimentos_e_pacientes ORDER BY data)\n",
        "    GROUP BY data, municipio, primeiro_estadiamento\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "dados_estad_municipio_mensal\\\n",
        "    .repartition(1)\\\n",
        "    .write\\\n",
        "    .format(\"delta\")\\\n",
        "    .mode(\"overwrite\")\\\n",
        "    .saveAsTable(f\"{database_name}.dados_municipios_mensal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mS4PmtP9dFfD"
      },
      "source": [
        "## 5.3 - Consolida dados por municipio mensal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQBFWXwT9ts3"
      },
      "outputs": [],
      "source": [
        "database_name = \"cancer_data\"\n",
        "dados_estad_municipio_mensal = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        mm.*,\n",
        "        COALESCE(em.numero_diagnosticos, 0) AS numero_diagnosticos\n",
        "    FROM dados_municipios_mensal mm\n",
        "    FULL OUTER JOIN diagnosticos_por_estadiamento_municipio em\n",
        "    ON mm.data = em.data\n",
        "    AND mm.municipio = em.municipio\n",
        "    AND mm.primeiro_estadiamento = em.primeiro_estadiamento\n",
        "\"\"\")\n",
        "\n",
        "dados_estad_municipio_mensal\\\n",
        "    .repartition(1)\\\n",
        "    .write\\\n",
        "    .format(\"delta\")\\\n",
        "    .mode(\"overwrite\")\\\n",
        "    .saveAsTable(f\"{database_name}.dados_estad_municipio_mensal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z0tYQaderL1"
      },
      "source": [
        "## 5.4 - Agregação por estado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3Ddz0QveqKk"
      },
      "outputs": [],
      "source": [
        "# Agregação por estado\n",
        "dados_estad_mensal = spark.sql(f\"\"\"\n",
        "    SELECT\n",
        "        estado,\n",
        "        data,\n",
        "        primeiro_estadiamento,\n",
        "        SUM(custo_estadiamento) AS custo_estadiamento,\n",
        "        SUM(numero_pacientes) AS numero_pacientes,\n",
        "        COUNT(DISTINCT(municipio)) AS numero_municipios,\n",
        "        SUM(obitos) AS obitos,\n",
        "        SUM(obito_futuro) AS obitos_futuros,\n",
        "        SUM(numero_procedimentos) AS numero_procedimentos,\n",
        "        SUM(numero_diagnosticos) AS numero_diagnosticos\n",
        "    FROM (\n",
        "        SELECT\n",
        "            cadastro_cidades.nome_uf AS estado,\n",
        "            mm.*\n",
        "        FROM cancer_data.dados_municipios_mensal mm\n",
        "        LEFT JOIN cancer_data.cadastro_municipios AS cadastro_cidades\n",
        "        ON int(mm.municipio) = int(cadastro_cidades.id / 10)\n",
        "        ORDER BY data\n",
        "    ) AS dados_estado\n",
        "    GROUP BY estado, data, primeiro_estadiamento\n",
        "\"\"\")\n",
        "\n",
        "# Salvando a tabela agregada por estado\n",
        "dados_estad_mensal\\\n",
        "    .repartition(1)\\\n",
        "    .write\\\n",
        "    .format(\"delta\")\\\n",
        "    .mode(\"overwrite\")\\\n",
        "    .saveAsTable(f\"{database_name}.dados_estados_mensal\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7YEI93j-zbA"
      },
      "source": [
        "# VI - Limpeza das tabelas delta, considerando 24 horas de retenção\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rafFyT9G-cFg"
      },
      "outputs": [],
      "source": [
        "vacuum_tables_from_database(\n",
        "        spark_session = spark,\n",
        "        database_name = database_name,\n",
        "        retention_hours = 24\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}