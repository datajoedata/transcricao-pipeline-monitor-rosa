{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datajoedata/transcricao-pipeline-monitor-rosa/blob/main/notebook_cancer_mama_kpis_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNviCQI6IDqh"
      },
      "source": [
        "# I - Configuração do ambiente\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqola1AcrdGm"
      },
      "source": [
        "## 1.1 - Montagem do drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "enmrocYf_raz",
        "outputId": "62ccb121-adda-430f-fd80-e9ddb8117f69"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         )\n\u001b[0;32m--> 277\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnBLAcn1ZEik"
      },
      "source": [
        "## 1.2 - Escolha do ambiente e carregamento\n",
        "São disponibilizadas duas opções de execução (dependendo da credencial utilizada)\n",
        " - Leitura: disponível para analises e usuários que não possuem permissão de escrita;\n",
        " - Escrita: disponível para atualização de bases (testes de desenvolvimento ou ambiente de produção)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fj3xNzUzZD8W",
        "outputId": "37ec23b8-2c68-40f3-e1a7-403deabb2825"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: SERVICE_ACCOUNT_USER=''\n",
            "env: SERVICE_ACCOUNT_JSON=''\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'shared_drive'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import os\n",
        "if os.path.isfile('/content/monitor-rosa-leitura.json'):\n",
        "    datalake_mode = 'leitura'\n",
        "    %env SERVICE_ACCOUNT_USER=acesso-leitura@monitor-rosa.iam.gserviceaccount.com\n",
        "    %env SERVICE_ACCOUNT_JSON=/content/monitor-rosa-leitura.json\n",
        "elif os.path.isfile('/content/monitor-rosa-escrita.json'):\n",
        "    datalake_mode = 'escrita'\n",
        "    %env SERVICE_ACCOUNT_USER=acesso-escrita@monitor-rosa.iam.gserviceaccount.com\n",
        "    %env SERVICE_ACCOUNT_JSON=/content/monitor-rosa-escrita.json\n",
        "else:\n",
        "    assert(os.path.isdir('/content/drive/Shareddrives/monitor-rosa-gold') == True)\n",
        "    datalake_mode = 'shared_drive'\n",
        "    %env SERVICE_ACCOUNT_USER=''\n",
        "    %env SERVICE_ACCOUNT_JSON=''\n",
        "datalake_mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxl5dgNHuCsK",
        "outputId": "f317839b-9b06-45a5-c052-809094597472"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'sus-kpis-analysis': No such file or directory\n",
            "Cloning into 'sus-kpis-analysis'...\n",
            "remote: Enumerating objects: 1768, done.\u001b[K\n",
            "remote: Counting objects: 100% (304/304), done.\u001b[K\n",
            "remote: Compressing objects: 100% (173/173), done.\u001b[K\n",
            "remote: Total 1768 (delta 126), reused 279 (delta 111), pack-reused 1464 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1768/1768), 4.72 MiB | 15.15 MiB/s, done.\n",
            "Resolving deltas: 100% (827/827), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -r sus-kpis-analysis\n",
        "!git clone https://github.com/heber-augusto/sus-kpis-analysis.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NYsSZynDK9K"
      },
      "source": [
        "## 1.3 - Instalação de libs Python, inicialização de variáveis de ambiente e configuração/instalação do Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v3Wqjxhu4Gq",
        "outputId": "e647d4f0-c358-41be-d467-34dde8c2f732"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark (from -r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 1))\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (from -r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 2)) (3.5.3)\n",
            "Collecting imagehash (from -r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 3))\n",
            "  Downloading ImageHash-4.3.1-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting delta-spark==2.4.0 (from -r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 4))\n",
            "  Downloading delta_spark-2.4.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (from -r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (2.8.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from -r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 6)) (2024.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 7)) (5.9.5)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from -r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 8)) (2.151.0)\n",
            "Collecting pyspark (from -r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 2))\n",
            "  Downloading pyspark-3.4.4.tar.gz (311.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from delta-spark==2.4.0->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 4)) (8.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 2)) (0.10.9.7)\n",
            "Collecting PyWavelets (from imagehash->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 3))\n",
            "  Downloading pywavelets-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imagehash->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 3)) (1.26.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from imagehash->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 3)) (11.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from imagehash->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (2.27.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (2.19.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (2.7.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (2.32.3)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 8)) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 8)) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 8)) (4.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (1.65.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (4.25.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (1.25.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (1.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 8)) (3.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.0.0->delta-spark==2.4.0->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 4)) (3.20.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (2024.8.30)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (0.6.1)\n",
            "Downloading delta_spark-2.4.0-py3-none-any.whl (20 kB)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Downloading ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.5/296.5 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pywavelets-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.4-py2.py3-none-any.whl size=311905460 sha256=04b5789b6b581ddbe306e7253b41d8d0abdab39ddfcb0c1a98d13623b66930ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/b2/e2/382460beb13a6ab29568e5e7807561604442b9556b385771ba\n",
            "Successfully built pyspark\n",
            "Installing collected packages: findspark, PyWavelets, pyspark, imagehash, delta-spark\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.5.3\n",
            "    Uninstalling pyspark-3.5.3:\n",
            "      Successfully uninstalled pyspark-3.5.3\n",
            "Successfully installed PyWavelets-1.7.0 delta-spark-2.4.0 findspark-2.0.1 imagehash-4.3.1 pyspark-3.4.4\n",
            "env: PYTHONHASHSEED=1234\n",
            "env: JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
            "env: SPARK_HOME=/content/spark-3.4.4-bin-hadoop3\n",
            "env: SPARK_VERSION=3.4.4\n",
            "starting spark env setup \n",
            "installing and downloading packages\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "setting enviroment variables\n",
            "spark env setup completed with success\n"
          ]
        }
      ],
      "source": [
        "!pip install -r /content/sus-kpis-analysis/sia/etls/requirements.txt\n",
        "\n",
        "%env PYTHONHASHSEED=1234\n",
        "%env JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
        "%env SPARK_HOME=/content/spark-3.4.4-bin-hadoop3\n",
        "%env SPARK_VERSION=3.4.4\n",
        "\n",
        "!source /content/sus-kpis-analysis/sia/etls/bin/setup_spark_env.sh '/content/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_O3K75fMh7K"
      },
      "outputs": [],
      "source": [
        "if datalake_mode != 'shared_drive':\n",
        "    %env XDG_CONFIG_HOME=/content/datalake\n",
        "    !source /content/sus-kpis-analysis/sia/etls/bin/install-google-drive-ocamlfuse.sh\n",
        "    !source /content/sus-kpis-analysis/sia/etls/bin/mount_google_drive_v2.sh '/content/datalake' $SERVICE_ACCOUNT_USER '0ABIY-a4qrdY9Uk9PVA' 'monitor-rosa-bronze' $SERVICE_ACCOUNT_JSON '/content'\n",
        "    !source /content/sus-kpis-analysis/sia/etls/bin/mount_google_drive_v2.sh '/content/datalake' $SERVICE_ACCOUNT_USER '0ALl0owLNr53oUk9PVA' 'monitor-rosa-silver' $SERVICE_ACCOUNT_JSON '/content'\n",
        "    !source /content/sus-kpis-analysis/sia/etls/bin/mount_google_drive_v2.sh '/content/datalake' $SERVICE_ACCOUNT_USER '0AMHp9pBeLvZiUk9PVA' 'monitor-rosa-gold' $SERVICE_ACCOUNT_JSON '/content'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT00Lo1mTJwu"
      },
      "source": [
        "## 1.4 - Inicializa variáveis de acesso ao delta lake criado no google storage\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> O caminho do warehouse pode ser alterado em caso de testes de escritas locais.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LLT8gGOcekpW",
        "outputId": "9107316e-64d4-49a7-dddf-edc98af9f201"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/spark-3.4.4-bin-hadoop3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "lake_prefix = \"temp-output\"\n",
        "\n",
        "if datalake_mode in ('leitura','shared_drive',):\n",
        "    warehouse_dir = f\"/content/datalake/{lake_prefix}/\"\n",
        "\n",
        "if datalake_mode == 'escrita':\n",
        "    warehouse_dir = f\"/content/datalake/\"\n",
        "\n",
        "spark_path = os.getenv('SPARK_HOME')\n",
        "spark_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpzMUcNg3k8d"
      },
      "source": [
        "## 1.5 - Inclusão da pasta do repositório no python path\n",
        "\n",
        "Procedimento permite que funções e classes presentes no repositório sejam utilizadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-dOrv8t0IMZ",
        "outputId": "3f9a0982-fcfd-4228-cd76-f008a50ff249"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content',\n",
              " '/env/python',\n",
              " '/usr/lib/python310.zip',\n",
              " '/usr/lib/python3.10',\n",
              " '/usr/lib/python3.10/lib-dynload',\n",
              " '',\n",
              " '/usr/local/lib/python3.10/dist-packages',\n",
              " '/usr/lib/python3/dist-packages',\n",
              " '/usr/local/lib/python3.10/dist-packages/IPython/extensions',\n",
              " '/usr/local/lib/python3.10/dist-packages/setuptools/_vendor',\n",
              " '/root/.ipython',\n",
              " '/content/sus-kpis-analysis']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('/content/sus-kpis-analysis')\n",
        "sys.path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptGWtMYSIXtw"
      },
      "source": [
        "## 1.6 - Importação de funções utilizadas pelo código"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYbxgrEQz3Sn"
      },
      "outputs": [],
      "source": [
        "from sia.etls.lib.catalog_loader import DeltaLakeDatabaseFsCreator, load_entire_catalog_fs, load_entire_catalog_fs_v2\n",
        "from sia.etls.lib.table_utilities import vacuum_tables_from_database, table_exists\n",
        "from sia.etls.lib.fs_spark_session import create_fs_spark_session\n",
        "from sia.etls.lib.bronze_files_utilities import get_pending_files_from_bronze\n",
        "from sia.etls.lib.delta_table_creators import ParquetToDelta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LWOtjtk33d6"
      },
      "source": [
        "## 1.7 - Cria Sessão Spark conectada ao Delta Lake presente no Google Storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETM5T_gkDvAP"
      },
      "outputs": [],
      "source": [
        "spark = create_fs_spark_session(\n",
        "    warehouse_dir=warehouse_dir,\n",
        "    spark_path=spark_path\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8lBJqvC4Cak"
      },
      "source": [
        "## 1.8 - Refresh do catálogo para utilizar consultas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvYjAMOAyNmj",
        "outputId": "e56d276b-17c5-466e-b028-f95c2b09460b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sia_bronze.db', 'cnes_bronze.db', 'sih_bronze.db', 'sim_bronze.db']\n",
            "Banco de dados sia_bronze criado.\n",
            "listando conteúdos do caminho /content/drive/Shareddrives/monitor-rosa-bronze/databases e database sia_bronze\n",
            "prefix: /content/drive/Shareddrives/monitor-rosa-bronze/databases/sia_bronze.db/\n",
            "table_list: ['ar', 'aq', 'pa', 'bi', 'am']\n",
            "Tabela ar criada\n",
            "Tabela ar criada com comando CREATE TABLE IF NOT EXISTS sia_bronze.ar USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-bronze/databases/sia_bronze.db/ar'\n",
            "Tabela aq criada\n",
            "Tabela aq criada com comando CREATE TABLE IF NOT EXISTS sia_bronze.aq USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-bronze/databases/sia_bronze.db/aq'\n",
            "Recriação das tabelas concluída.\n",
            "Banco de dados cnes_bronze criado.\n",
            "listando conteúdos do caminho /content/drive/Shareddrives/monitor-rosa-bronze/databases e database cnes_bronze\n",
            "prefix: /content/drive/Shareddrives/monitor-rosa-bronze/databases/cnes_bronze.db/\n",
            "table_list: ['dc', 'lt', 'ep', 'pf', 'eq', 'hb', 'in', 'rc', 'sr']\n",
            "Recriação das tabelas concluída.\n",
            "Banco de dados sih_bronze criado.\n",
            "listando conteúdos do caminho /content/drive/Shareddrives/monitor-rosa-bronze/databases e database sih_bronze\n",
            "prefix: /content/drive/Shareddrives/monitor-rosa-bronze/databases/sih_bronze.db/\n",
            "table_list: ['rd']\n",
            "Recriação das tabelas concluída.\n",
            "Banco de dados sim_bronze criado.\n",
            "listando conteúdos do caminho /content/drive/Shareddrives/monitor-rosa-bronze/databases e database sim_bronze\n",
            "prefix: /content/drive/Shareddrives/monitor-rosa-bronze/databases/sim_bronze.db/\n",
            "table_list: ['dores']\n",
            "Recriação das tabelas concluída.\n",
            "['cancer_data.db']\n",
            "Banco de dados cancer_data criado.\n",
            "listando conteúdos do caminho /content/drive/Shareddrives/monitor-rosa-silver/databases e database cancer_data\n",
            "prefix: /content/drive/Shareddrives/monitor-rosa-silver/databases/cancer_data.db/\n",
            "table_list: ['aq_filtered', 'ar_filtered', 'dados_estados_mensal', 'dados_municipios_mensal', 'pacientes', 'procedimentos', 'procedimentos_e_pacientes', 'demografia_municipios', 'cadastro_municipios']\n",
            "Tabela demografia_municipios criada\n",
            "Tabela demografia_municipios criada com comando CREATE TABLE IF NOT EXISTS cancer_data.demografia_municipios USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-silver/databases/cancer_data.db/demografia_municipios'\n",
            "Tabela cadastro_municipios criada\n",
            "Tabela cadastro_municipios criada com comando CREATE TABLE IF NOT EXISTS cancer_data.cadastro_municipios USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-silver/databases/cancer_data.db/cadastro_municipios'\n",
            "Recriação das tabelas concluída.\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "zone_names = ['monitor-rosa-bronze', 'monitor-rosa-silver', 'monitor-rosa-gold']\n",
        "if datalake_mode in ('leitura', 'escrita'):\n",
        "    zone_paths = [f'/content/datalake/{zone_name}/databases' for zone_name in zone_names]\n",
        "else:\n",
        "    zone_paths = [f'/content/drive/Shareddrives/{zone_name}/databases' for zone_name in zone_names]\n",
        "\n",
        "\n",
        "# carrega catalogo de banco de dados, na zona bronze\n",
        "database_filter = None #['cnes_bronze.db',]\n",
        "table_filter = ['sia_bronze.ar','sia_bronze.aq','cancer_data.cadastro_municipios', 'cancer_data.demografia_municipios']\n",
        "\n",
        "for databases_path in zone_paths:\n",
        "    load_entire_catalog_fs_v2(\n",
        "        spark_session = spark,\n",
        "        databases_path = databases_path,\n",
        "        use_db_folder_path=(datalake_mode == 'escrita'),\n",
        "        database_filter=database_filter,\n",
        "        table_filter=table_filter\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeQru0rTIfct"
      },
      "source": [
        "# II - Exemplo de como listar bancos e tabelas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJybgzFHU5as",
        "outputId": "5b4efdc6-31b1-40df-cf87-98c55773c94d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|  namespace|\n",
            "+-----------+\n",
            "|cancer_data|\n",
            "|cnes_bronze|\n",
            "|    default|\n",
            "| sia_bronze|\n",
            "| sih_bronze|\n",
            "| sim_bronze|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "databases = spark.sql(f\"SHOW DATABASES;\")\n",
        "databases.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYbmd2NNH3iZ",
        "outputId": "098e258a-5b25-48b9-fedf-6e38f907b3b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------------------+-----------+\n",
            "|namespace  |tableName            |isTemporary|\n",
            "+-----------+---------------------+-----------+\n",
            "|cancer_data|cadastro_municipios  |false      |\n",
            "|cancer_data|demografia_municipios|false      |\n",
            "+-----------+---------------------+-----------+\n",
            "\n",
            "+---------+---------+-----------+\n",
            "|namespace|tableName|isTemporary|\n",
            "+---------+---------+-----------+\n",
            "+---------+---------+-----------+\n",
            "\n",
            "+---------+---------+-----------+\n",
            "|namespace|tableName|isTemporary|\n",
            "+---------+---------+-----------+\n",
            "+---------+---------+-----------+\n",
            "\n",
            "+----------+---------+-----------+\n",
            "|namespace |tableName|isTemporary|\n",
            "+----------+---------+-----------+\n",
            "|sia_bronze|aq       |false      |\n",
            "|sia_bronze|ar       |false      |\n",
            "+----------+---------+-----------+\n",
            "\n",
            "+---------+---------+-----------+\n",
            "|namespace|tableName|isTemporary|\n",
            "+---------+---------+-----------+\n",
            "+---------+---------+-----------+\n",
            "\n",
            "+---------+---------+-----------+\n",
            "|namespace|tableName|isTemporary|\n",
            "+---------+---------+-----------+\n",
            "+---------+---------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for row in databases.collect():\n",
        "    spark.sql(f\"SHOW TABLES FROM {row['namespace']};\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy88IuwDyjFQ"
      },
      "source": [
        "# III - Extração e Filtragem:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa-XADxhEWvE"
      },
      "source": [
        "## 3.1 - Definindo utils e filtros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_joP1K-LEbIQ"
      },
      "outputs": [],
      "source": [
        "# Função para gerar consultas SQL\n",
        "def get_select_all_query(table_name, where_clause=''):\n",
        "    return f\"\"\"\n",
        "    SELECT\n",
        "        *\n",
        "    FROM {table_name}\n",
        "    {where_clause}\n",
        "    \"\"\"\n",
        "\n",
        "# Função para executar consultas SQL no Spark\n",
        "def run_sql_query(sql_query):\n",
        "    return spark.sql(sql_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rfb1iT7fEnCh",
        "outputId": "1ce234e7-d560-4a0c-eabf-bc6ef8db1956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('0201010569','0201010585','0201010607','0203010035','0203010043','0203020065','0203020073','0205020097','0208090037','0204030030','0204030188')\n",
            "('C500','C501','C502','C503','C504','C505','C506','C508','C509')\n"
          ]
        }
      ],
      "source": [
        "# Filtro pelo CID\n",
        "cid_filter = ['C500', 'C501', 'C502', 'C503', 'C504', 'C505', 'C506', 'C508', 'C509']\n",
        "\n",
        "cid_filter = f\"\"\"({','.join([f\"'{cid_id}'\" for cid_id in cid_filter])})\"\"\"\n",
        "\n",
        "# Dicionário e filtro de procedimentos\n",
        "proc_id_dict = {\n",
        "    '0201010569': 'BIOPSIA/EXERESE DE NÓDULO DE MAMA',\n",
        "    '0201010585': 'PUNÇÃO ASPIRATIVA DE MAMA POR AGULHA FINA',\n",
        "    '0201010607': 'PUNÇÃO DE MAMA POR AGULHA GROSSA',\n",
        "    '0203010035': 'EXAME DE CITOLOGIA (EXCETO CERVICO-VAGINAL E DE MAMA)',\n",
        "    '0203010043': 'EXAME CITOPATOLOGICO DE MAMA',\n",
        "    '0203020065': 'EXAME ANATOMOPATOLOGICO DE MAMA - BIOPSIA',\n",
        "    '0203020073': 'EXAME ANATOMOPATOLOGICO DE MAMA - PECA CIRURGICA',\n",
        "    '0205020097': 'ULTRASSONOGRAFIA MAMARIA BILATERAL',\n",
        "    '0208090037': 'CINTILOGRAFIA DE MAMA (BILATERAL)',\n",
        "    '0204030030': 'MAMOGRAFIA',\n",
        "    '0204030188': 'MAMOGRAFIA BILATERAL PARA RASTREAMENTO'\n",
        "}\n",
        "proc_id_filter = f\"\"\"({','.join([f\"'{proc_id}'\" for proc_id in proc_id_dict.keys()])})\"\"\"\n",
        "\n",
        "# Mostrar os filtros\n",
        "print(proc_id_filter)\n",
        "print(cid_filter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeIwwmsSHTzv"
      },
      "source": [
        "## 3.2 - Carrega tabela SIA.AR filtrando dados de câncer de mama e procedimentos de interesse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t36KwFBRtY1k"
      },
      "outputs": [],
      "source": [
        "sql_query_aq = get_select_all_query(\n",
        "    table_name= 'sia_bronze.ar',\n",
        "    where_clause=f\"WHERE AP_CIDPRI IN {cid_filter} AND AP_MVM = '202405'\"\n",
        ")\n",
        "\n",
        "cancer_ar_filtered = run_sql_query(sql_query_aq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx_GDZq62XTU"
      },
      "source": [
        "Escreve a tabela em .cancer_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLCBiUZQ2R2t"
      },
      "outputs": [],
      "source": [
        "cancer_ar_filtered\\\n",
        "      .repartition(1)\\\n",
        "      .write\\\n",
        "      .format(\"delta\")\\\n",
        "      .mode(\"overwrite\")\\\n",
        "      .saveAsTable(\"cancer_data.ar_filtered\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjhvBzKogaT9"
      },
      "source": [
        "## 3.3 - Carrega tabela SIA.AQ filtrando dados de câncer de mama e procedimentos de interesse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bIQolpgc_a9"
      },
      "outputs": [],
      "source": [
        "sql_query_aq = get_select_all_query(\n",
        "    table_name= 'sia_bronze.aq',\n",
        "    where_clause=f\"WHERE AP_CIDPRI IN {cid_filter} AND AP_MVM = '202405'\"\n",
        ")\n",
        "\n",
        "cancer_aq_filtered = run_sql_query(sql_query_aq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShU0d5Fv2agE"
      },
      "source": [
        "Escreve a tabela em .cancer_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-Jga0Lpeq32"
      },
      "outputs": [],
      "source": [
        "cancer_aq_filtered\\\n",
        "      .repartition(1)\\\n",
        "      .write\\\n",
        "      .format(\"delta\")\\\n",
        "      .mode(\"overwrite\")\\\n",
        "      .saveAsTable(\"cancer_data.aq_filtered\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ncymjIK6NlR"
      },
      "source": [
        "# IV - Processamento dos Dados dos Pacientes e Procedimentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn0xcaY-j3E2"
      },
      "source": [
        "## 4.1 - Cria dados consolidados de pacientes e procedimentos (quimio e radioterapia)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mx39mQ5w4S6"
      },
      "source": [
        "Radioterapia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozvlA8On_dHJ",
        "outputId": "bcd0ab1d-6a8b-4a20-ab57-e050a00e97a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------------+------------+------+-----+---------+\n",
            "|  data|       paciente|estadiamento| custo|obito|municipio|\n",
            "+------+---------------+------------+------+-----+---------+\n",
            "|202405|{{|}{}}}|           1|5904.0|    0|   355030|\n",
            "|202405|{{}}}}~~~|           3|5904.0|    0|   352310|\n",
            "|202404|{|{}||           2|   0.0|    0|   355030|\n",
            "+------+---------------+------------+------+-----+---------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cancer_ar_res = spark.sql(f\"\"\"\n",
        "SELECT\n",
        "    AP_CMP as data,\n",
        "    AP_CNSPCN as paciente,\n",
        "    AR_ESTADI as estadiamento,\n",
        "    DOUBLE(AP_VL_AP)  as custo,\n",
        "    INT(AP_OBITO) as obito,\n",
        "    AP_MUNPCN as municipio\n",
        "FROM cancer_data.ar_filtered\n",
        "\"\"\")\n",
        "cancer_ar_res.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlF0pvjRw74j"
      },
      "source": [
        "Quimio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Dyzqz4rGhmG",
        "outputId": "4f04274d-c369-4a8b-e3d3-5757c6f0a8d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------------+------------+------+-----+---------+\n",
            "|  data|       paciente|estadiamento| custo|obito|municipio|\n",
            "+------+---------------+------------+------+-----+---------+\n",
            "|202404|{{}{|{|           3|1400.0|    0|   351640|\n",
            "|202404|{{||~}||           3|1400.0|    0|   355030|\n",
            "|202405|{}~{|{||||           4|1400.0|    0|   355220|\n",
            "+------+---------------+------------+------+-----+---------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cancer_aq_res = spark.sql(f\"\"\"\n",
        "SELECT\n",
        "    AP_CMP as data,\n",
        "    AP_CNSPCN as paciente,\n",
        "    AQ_ESTADI as estadiamento,\n",
        "    DOUBLE(AP_VL_AP)  as custo,\n",
        "    INT(AP_OBITO) as obito,\n",
        "    AP_MUNPCN as municipio\n",
        "FROM cancer_data.aq_filtered\n",
        "\"\"\")\n",
        "cancer_aq_res.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOgAevcCNa07"
      },
      "source": [
        "## 4.2 - Unifica os dados de radio e quimio consolidados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94M3aPpRNZeT"
      },
      "outputs": [],
      "source": [
        "df_union = cancer_aq_res.union(cancer_ar_res)\n",
        "\n",
        "df_union.createOrReplaceTempView(\"cancer_ordered\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYZcjpFJVtbT"
      },
      "outputs": [],
      "source": [
        "df_union\\\n",
        "  .repartition(1)\\\n",
        "  .write\\\n",
        "  .format(\"delta\")\\\n",
        "  .mode(\"overwrite\")\\\n",
        "  .saveAsTable(f\"cancer_data.procedimentos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s588c0EgN5pU"
      },
      "source": [
        "## 4.3 - Consolidando os dados por paciente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kymNuaRsg87T"
      },
      "outputs": [],
      "source": [
        "res_consolidado = spark.sql(\"\"\"\n",
        "SELECT\n",
        "    paciente,\n",
        "    FIRST(data) as data_primeiro_estadiamento,\n",
        "    LAST(data) as data_ultimo_estadiamento,\n",
        "    COUNT(1) as numero_procedimentos,\n",
        "    FIRST(estadiamento) as primeiro_estadiamento,\n",
        "    LAST(estadiamento) as ultimo_estadiamento,\n",
        "    MAX (estadiamento) as maior_estadiamento,\n",
        "    MIN (estadiamento) as menor_estadiamento,\n",
        "    SUM(custo) as custo_total,\n",
        "    MAX(obito) as indicacao_obito,\n",
        "    FIRST(municipio) as primeiro_municipio,\n",
        "    LAST(municipio) as ultimo_municipio\n",
        "FROM (SELECT * FROM cancer_ordered ORDER BY paciente, data)\n",
        "GROUP BY paciente\n",
        "\"\"\")\n",
        "\n",
        "res_consolidado\\\n",
        "  .repartition(1)\\\n",
        "  .write\\\n",
        "  .format(\"delta\")\\\n",
        "  .mode(\"overwrite\")\\\n",
        "  .saveAsTable(f\"cancer_data.pacientes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31EFS5RgWrPr"
      },
      "source": [
        "## 4.4 - Procedimentos e Pacientes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2319Trei2iY"
      },
      "outputs": [],
      "source": [
        "procedimentos_e_pacientes = spark.sql(f\"\"\"\n",
        "  SELECT\n",
        "      c.*,\n",
        "      p.data_primeiro_estadiamento,\n",
        "      p.data_ultimo_estadiamento,\n",
        "      p.primeiro_estadiamento,\n",
        "      p.maior_estadiamento,\n",
        "      p.ultimo_estadiamento,\n",
        "      p.custo_total,\n",
        "      p.primeiro_municipio,\n",
        "      p.ultimo_municipio,\n",
        "      p.indicacao_obito\n",
        "  FROM cancer_data.procedimentos AS c\n",
        "  FULL OUTER JOIN cancer_data.pacientes AS p\n",
        "  ON c.paciente = p.paciente\n",
        "\"\"\")\n",
        "\n",
        "procedimentos_e_pacientes\\\n",
        "  .repartition(1)\\\n",
        "  .write\\\n",
        "  .format(\"delta\")\\\n",
        "  .mode(\"overwrite\")\\\n",
        "  .saveAsTable(\"cancer_data.procedimentos_e_pacientes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZyJAon8CaMw"
      },
      "source": [
        "# V - Agregação por Município e Estado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRM6BlAT9xPY"
      },
      "source": [
        "## 5.1 - Consolida dados por municipio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60A9MQRpaCZk"
      },
      "outputs": [],
      "source": [
        "database_name = \"cancer_data\"\n",
        "\n",
        "diagnosticos_por_estadiamento_municipio_df = spark.sql(f\"\"\"\n",
        "    SELECT\n",
        "        primeiro_estadiamento,\n",
        "        data_primeiro_estadiamento AS data,\n",
        "        primeiro_municipio AS municipio,\n",
        "        COUNT(DISTINCT(paciente)) AS numero_diagnosticos\n",
        "    FROM {database_name}.pacientes\n",
        "    WHERE primeiro_estadiamento != ''\n",
        "    GROUP BY primeiro_estadiamento, data_primeiro_estadiamento, primeiro_municipio\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "\n",
        "diagnosticos_por_estadiamento_municipio_df.createOrReplaceTempView(\"diagnosticos_por_estadiamento_municipio\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbZFX_cNdwPt"
      },
      "source": [
        "## 5.2 - Consolida dados mensais por municipio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLwtdxLCdrTN"
      },
      "outputs": [],
      "source": [
        "dados_estad_municipio_mensal_df = spark.sql(f\"\"\"\n",
        "    SELECT\n",
        "        data,\n",
        "        municipio,\n",
        "        primeiro_estadiamento,\n",
        "        SUM(custo) AS custo_estadiamento,\n",
        "        COUNT(DISTINCT(paciente)) AS numero_pacientes,\n",
        "        SUM(DISTINCT(obito)) AS obitos,\n",
        "        SUM(DISTINCT(indicacao_obito)) AS obito_futuro,\n",
        "        COUNT(1) AS numero_procedimentos\n",
        "    FROM\n",
        "        (SELECT * FROM {database_name}.procedimentos_e_pacientes ORDER BY data)\n",
        "    GROUP BY data, municipio, primeiro_estadiamento\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "\n",
        "dados_estad_municipio_mensal_df.createOrReplaceTempView(\"dados_municipios_mensal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mS4PmtP9dFfD"
      },
      "source": [
        "## 5.3 - Consolida dados por municipio mensal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQBFWXwT9ts3"
      },
      "outputs": [],
      "source": [
        "dados_estad_municipio_mensal = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        mm.*,\n",
        "        COALESCE(em.numero_diagnosticos, 0) AS numero_diagnosticos\n",
        "    FROM dados_municipios_mensal mm\n",
        "    FULL OUTER JOIN diagnosticos_por_estadiamento_municipio em\n",
        "    ON mm.data = em.data\n",
        "    AND mm.municipio = em.municipio\n",
        "    AND mm.primeiro_estadiamento = em.primeiro_estadiamento\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "\n",
        "dados_estad_municipio_mensal\\\n",
        "    .repartition(1)\\\n",
        "    .write\\\n",
        "    .format(\"delta\")\\\n",
        "    .mode(\"overwrite\")\\\n",
        "    .saveAsTable(f\"{database_name}.dados_estad_municipio_mensal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z0tYQaderL1"
      },
      "source": [
        "## 5.4 - Agregação por estado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3Ddz0QveqKk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "outputId": "f169baa4-27e8-43f6-f971-1abb2b9fc3a5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `cancer_data`.`dados_municipios_mensal` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 17 pos 13;\n'Aggregate ['estado, 'data, 'primeiro_estadiamento], ['estado, 'data, 'primeiro_estadiamento, 'SUM('custo_estadiamento) AS custo_estadiamento#9639, 'SUM('numero_pacientes) AS numero_pacientes#9640, 'COUNT(distinct 'municipio) AS numero_municipios#9641, 'SUM('obitos) AS obitos#9642, 'SUM('obito_futuro) AS obitos_futuros#9643, 'SUM('numero_procedimentos) AS numero_procedimentos#9644, 'SUM('numero_diagnosticos) AS numero_diagnosticos#9645]\n+- 'SubqueryAlias dados_estado\n   +- 'Sort ['data ASC NULLS FIRST], true\n      +- 'Project ['cadastro_cidades.nome_uf AS estado#9638, mm.*]\n         +- 'Join LeftOuter, ('int('mm.municipio) = 'int(('cadastro_cidades.id / 10)))\n            :- 'SubqueryAlias mm\n            :  +- 'UnresolvedRelation [cancer_data, dados_municipios_mensal], [], false\n            +- SubqueryAlias cadastro_cidades\n               +- SubqueryAlias spark_catalog.cancer_data.cadastro_municipios\n                  +- Relation spark_catalog.cancer_data.cadastro_municipios[id#9646,nome#9647,id_uf#9648,nome_uf#9649] parquet\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-51ed15e56d54>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m dados_estad_mensal = spark.sql(f\"\"\"\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mSELECT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mestado\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprimeiro_estadiamento\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m             \u001b[0mlitArgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1440\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlitArgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1441\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `cancer_data`.`dados_municipios_mensal` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 17 pos 13;\n'Aggregate ['estado, 'data, 'primeiro_estadiamento], ['estado, 'data, 'primeiro_estadiamento, 'SUM('custo_estadiamento) AS custo_estadiamento#9639, 'SUM('numero_pacientes) AS numero_pacientes#9640, 'COUNT(distinct 'municipio) AS numero_municipios#9641, 'SUM('obitos) AS obitos#9642, 'SUM('obito_futuro) AS obitos_futuros#9643, 'SUM('numero_procedimentos) AS numero_procedimentos#9644, 'SUM('numero_diagnosticos) AS numero_diagnosticos#9645]\n+- 'SubqueryAlias dados_estado\n   +- 'Sort ['data ASC NULLS FIRST], true\n      +- 'Project ['cadastro_cidades.nome_uf AS estado#9638, mm.*]\n         +- 'Join LeftOuter, ('int('mm.municipio) = 'int(('cadastro_cidades.id / 10)))\n            :- 'SubqueryAlias mm\n            :  +- 'UnresolvedRelation [cancer_data, dados_municipios_mensal], [], false\n            +- SubqueryAlias cadastro_cidades\n               +- SubqueryAlias spark_catalog.cancer_data.cadastro_municipios\n                  +- Relation spark_catalog.cancer_data.cadastro_municipios[id#9646,nome#9647,id_uf#9648,nome_uf#9649] parquet\n"
          ]
        }
      ],
      "source": [
        "dados_estad_mensal = spark.sql(f\"\"\"\n",
        "    SELECT\n",
        "        estado,\n",
        "        data,\n",
        "        primeiro_estadiamento,\n",
        "        SUM(custo_estadiamento) AS custo_estadiamento,\n",
        "        SUM(numero_pacientes) AS numero_pacientes,\n",
        "        COUNT(DISTINCT(municipio)) AS numero_municipios,\n",
        "        SUM(obitos) AS obitos,\n",
        "        SUM(obito_futuro) AS obitos_futuros,\n",
        "        SUM(numero_procedimentos) AS numero_procedimentos,\n",
        "        SUM(numero_diagnosticos) AS numero_diagnosticos\n",
        "    FROM (\n",
        "        SELECT\n",
        "            cadastro_cidades.nome_uf AS estado,\n",
        "            mm.*\n",
        "        FROM cancer_data.dados_municipios_mensal mm\n",
        "        LEFT JOIN cancer_data.cadastro_municipios AS cadastro_cidades\n",
        "        ON int(mm.municipio) = int(cadastro_cidades.id / 10)\n",
        "        ORDER BY data\n",
        "    ) AS dados_estado\n",
        "    GROUP BY estado, data, primeiro_estadiamento\n",
        "\"\"\")\n",
        "\n",
        "# Salvando a tabela agregada por estado\n",
        "dados_estad_mensal\\\n",
        "    .repartition(1)\\\n",
        "    .write\\\n",
        "    .format(\"delta\")\\\n",
        "    .mode(\"overwrite\")\\\n",
        "    .saveAsTable(f\"{database_name}.dados_estados_mensal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7YEI93j-zbA"
      },
      "source": [
        "# VI - Limpeza das tabelas delta, considerando 24 horas de retenção\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rafFyT9G-cFg"
      },
      "outputs": [],
      "source": [
        "vacuum_tables_from_database(\n",
        "        spark_session = spark,\n",
        "        database_name = database_name,\n",
        "        retention_hours = 24\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}